div.content
	div.col-md-10
		div.row(style="margin-bottom:10px")
			p(style="font-size:2em") Getting Started with Tensorflow

		div.row(style="margin-bottom:10px")
			p(style="font-size:1.3em") Some of the current projects I'm working on are about how deep learning can be used to develop biomedical applications. For people unfamiliar with the term, deep learning means using large amounts of data to find values for the tunable parameters in a large neural network. The goal is to find values that will cause the neural network to perform well on the task we have in mind. For example, finding blood vessels in MRI data. 

			p(style="font-size:1.3em") In doing this kind of research, you inevitably end up having to write a lot of code to test out different neural network architectures, different optimization settings and many other things. To make things easier you typically use a code library that already has a lot of the functionality you need. The point is, having the right library can make all the difference between making your work easy, or difficult.

			p(style="font-size:1.3em") Up to now I have been using the <a href="http://caffe.berkeleyvision.org/"> Caffe</a> library for most of my projects. Caffe is a neural network library written in C++ with support for training neural networks on GPUs. As a result it's fast and does a good job of making it possible to train some serious networks. However, I find it hard to write flexible code with Caffe. So whenever I want to try a lot of different experiments I end up having to write a lot of different code which is cumbersome. 

			p(style="font-size:1.3em")  Google recently announced the release of their new open-source software framework <a href="https://www.tensorflow.org/">Tensorflow</a> with the goal of, among others, making deep learning research easier. Because of my frustrations with Caffe, I decided it would be a good time to experiment with Tensorflow. I think it's a very cool framework and with this blog post I want to share what I have learned about how Tensorflow works. That way, if you ever decide to look into it, you will hopefully have a head start.

			p(style="font-size:1.8em") How does Tensorflow work? 

			p(style="font-size:1.3em") Tensorflow's functionality can be accessed via both a python and C++ library. However, at the time of this writing the python library is the most convenient way to use Tensorflow, so that is what I will focus on in this post.

			p(style="font-size:1.3em") Conceptually, Tensorflow lets us write our programs, or algorithms, as a series of operations, or ops, that are strung together to form a computational graph. A computational graph is basically like a flow chart, where we start with a few blocks that then lead into other blocks, all the way until we reach the end of the chart. An example of a computational graph looks like this:

			img(src="/img/comp_graph.png" style="width:600px;height:500px;")

			p(style="font-size:1.3em") In this case we have combined some simple mathematical operations to compute <b>z=d*c=(a+b)*c</b>. Now this is a very simple example and normally you would not need a library to do this, but when we are dealing with more complicated operations, it is very useful to be able to systematically structure our code in this way. So when using Tensorflow, most of our work involves breaking down our code into smaller operations and stringing these together to get the desired output. Tensorflow then takes care of storing the state of any variables in the graph and feeds them through the specified operations.

			p(style="font-size:1.3em") Another useful feature of Tensorflow is that it can compute the gradient of a specified cost function with respect to the parameters in our graph. We can make use of this feature to optimize our graphs for machine learning applications.

			p(style="font-size:1.8em") Installing Tensorflow 

			p(style="font-size:1.3em") Naturally the first thing to do is to figure out how to install Tensorflow.  One of the ways to get Tensorflow is to install it as a Python package. This is probably also the easiest way to install. Do note however that if you want to use Tensorflow with a GPU you will need the latest version of CUDA and CUDNN installed on your system beforehand. Once the dependencies are installed you can get Tensorflow by using Python's pip install command. The official documentation lists the exact commands <a href="https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#pip-installation">here</a>

			p(style="font-size:1.3em") If, like me, you do not have the latest version of CUDA installed and do not want to install it, then you will have to download and build the Tensorflow python package from source. Unfortuantley this is a bit more complicated. It basically requires you to install Google's <a href="http://bazel.io/docs/install.html">Bazel build system</a> which you can then use to build the Tensorflow packages. Again the official documentation provides good instructions <a href="https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#installing-from-sources">here</a>. I did get stuck at a few points and found looking at the automatic install script, written by Erik Bernhardsson, <a href="https://gist.github.com/erikbern/78ba519b97b440e10640">here</a> to be useful to see what I was doing wrong.


			p(style="font-size:1.8em") Example: Linear Regression 			

			p(style="font-size:1.3em") To get a better idea of how Tensorflow works and to see how to call some of its functions let's cover a simple example. The example I will cover is how to perform linear regression using Tensorflow. The full code can be found <a href="https://github.com/gmaher/tensorflow_examples/blob/master/linear_regression.py">here</a>.

			p(style="font-size:1.3em") The basic premise of linear regression is that we have data points \(x^{(i)}\) with associated output variables \(y^{(i)}\) and we would like to estimate the parameters, \(w\) and \(b\) to produce a predictor:

			$$\hat{y}^{(i)} = w^Tx^{(i)} + b$$

			p(style="font-size:1.3em") such that \(\hat{y}^{(i)}\) is close to \(y^{(i)}\). To get this working in Tensorflow we have to set up our computational graph to compute \(\hat{y}^{(i)}\) and then call some of Tensorflow's optimization functions to find the best values for \(w\) and \(b\). The code for doing the first part is:

			pre 
				code.
					import tensorflow as tf
					import numpy as np

					#create the data and add gaussian noise to the output Y
					#note need the float 32 because numpy default is float64
					#and tensorflow default is float32
					X1 = np.random.rand(1,100).astype(np.float32)
					X2 = X1**2
					X = np.vstack((X1,X2))
					Y = 2*X[0,:] + 0.2*X[1,:] + 1 + 0.05*np.random.normal(size=100)

					#set up the tensorflow graph
					W = tf.Variable(tf.random_uniform([2,1],0,4.0))
					b = tf.Variable(tf.zeros([1]))


					yhat = tf.matmul(tf.transpose(W),X) + b


			p(style="font-size:1.3em") The first thing to note is that since we are not optimizing \(x\) or \(y\), they do not have to be a part of our graph. So we can just declare them as numpy arrays. Given \(x\), here we are trying to find \(w\) and \(b\) to fit \(y=2x+0.2x^2 +1 +\epsilon\) where \(\epsilon\) is some added noise to simulate measurement error.

			p(style="font-size:1.3em") The next step is to have Tensorflow declare \(w\) and \(b\) as parts of our computational graph. This is done using the command <b>tf.Variable()</b>. What this command does is tell Tensorflow to add a variable, or tensor, to our graph (these are like the a, b and c nodes in the picture above). We can also provide an input argument that tells Tensorflow what dimensions to give the tensor and with what values to initialize it. So for example the command <b>tf.Variable(tf.random_uniform([2,1],0,4.0))</b> tells Tensorflow to initialize a tensor with 2 rows and 1 column with initial values randomly selected from the uniform distribution between 0 and 4.

			p(style="font-size:1.3em") Once the necessary variables are declared, we then have to get Tensorflow to add the required mathematical operations, or ops, to our graph. Here this is done with the code <b>yhat = tf.matmul(tf.transpose(W),X) + b</b>. This tells Tensorflow to transpose \(w\), then compute the matrix multiplication \(w^Tx\), then add \(b\) and store the result in \(\hat{y}\), which becomes a new tensor node in our graph.

			p(style="font-size:1.3em") We now have all the required ops set up to calculate \(\hat{y}\) but we still need to get Tensorflow to optimize \(w\) and \(b\) so that \(\hat{y}\) is close to \(y\). The way this is done in Tensorflow is that we define a loss function to measure the error. Then we create an optimizer object and get that optimizer object to minimize the loss function! The code that does all that is:

			pre
				code.
					#set up loss function to minimize
					loss = tf.reduce_mean(tf.square(Y-yhat)) + 0.05*b*b + 0.05*tf.reduce_mean(tf.square(W)) 
					optimizer = tf.train.GradientDescentOptimizer(0.05)
					train = optimizer.minimize(loss)

			p(style="font-size:1.3em") The loss function we're using is the regularized \(L_2\) error:

			$$L(\hat{y},y) = \frac{1}{N}\sum_{i=1}^N(\hat{y}^{(i)}-y^{(i)})^2 + 0.05b^2 + 0.05||w||_2^2$$

			p(style="font-size:1.3em") we set this up as an op in Tensorflow using the code <b>loss = tf.reduce_mean(tf.square(Y-yhat)) + 0.45*b*b + 0.45*tf.reduce_mean(tf.square(W))</b>. The command <b>tf.square()</b> computes the elementwise square of a tensor and the command <b>tf.reduce_mean()</b> computes the average of all the values contained in a tensor. The last two lines of code show how easy it is to do optimization using Tensorflow. We simply create a <b>tf.train.GradientDescentOptimizer</b> object and then define the op <b>train</b> as a call to <b>optimizer.minimize(loss)</b> which will run one step of gradient descent. Notice that we did not have to tell Tensorflow anything about which variables to optimize, it already knows that \(w\) and \(b\) are the only nodes in the graph that affect the loss and so will automatically optimize their values.

			p(style="font-size:1.3em") Now is the time to note that so far we have only constructed our graph and Tensorflow has not actually computed anything for us. This is because Tensorflow separates the construction of an op, from actually running that op. The code that actually runs the ops in our graph is:

			pre
				code.
					init = tf.initialize_all_variables()

					sess = tf.Session()
					sess.run(init)

					#run the gradient descent by repeatedly calling sess.run(train)
					#this runs the operation specified by the train variable above
					for step in xrange(2001):
						sess.run(train)
						if step % 20 == 0:
							print(step, sess.run(W), sess.run(b), sess.run(loss))

			p(style="font-size:1.3em") We first create an op <b>init</b> which, when run, will cause Tensorflow to initialize all the variables in the graph. Next to be able to run our ops we have to define a Session object, and it is with this session object that we can actually run ops. For example the code <b>sess.run(init)</b> will tell Tensorflow to actually execute the init op. After initializing everything, we can perform optimization by repeatedly calling the train op, this is why we call <b>sess.run(train)</b> in a for loop. Also note that to get the value of any tensors we can simply call <b>sess.run(tensor name)</b> as is done in the print statement.

			p(style="font-size:1.3em") That completes our linear regression example. Running the full code, we get the plot shown below, which shows that our optimization worked and produced parameters that resulted in a good fit.

			img(src="/img/plot.png" style="width:500px;height:400px;")

			p(style="font-size:1.8em") Closing Remarks

			p(style="font-size:1.3em") Hopefully this post has helped you become more familiar with Tensorflow and how to get started actually writing Tensorflow code. The basic idea to remember is that we define our code as a computational graph by getting Tensorflow to add ops to our graph. We then run those ops by creating a session object and calling <b>session_object_name.run(ops)</b>. Tensorflow provides a lot of different ops out of the box, and the <a href="https://www.tensorflow.org/versions/r0.8/api_docs/index.html">official documentation</a> provides a good overview of what they do and how to use them.

			p(style="font-size:1.3em") There are more cool features that Tensorflow provides that I was not able to describe in this post. In particular the Neural Network capabilities of Tensorflow are very interesting. Tensorflow even provides an interface called Tensorboard that you can use to monitor your code, which makes it easier to debug and see what is going on. I will be writing about both of these aspects of Tensorflow in another blog post, so stay tuned!

			p(style="font-size:1.3em") Gabriel
			div(ng-init='MathJax.Hub.Queue(["Typeset",MathJax.Hub]);')